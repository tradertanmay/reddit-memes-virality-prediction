{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install fer\n",
    "!pip install easyocr\n",
    "!pip install transformers"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"ha2dkfd4fspe0ILZgWkRlu",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import os\n",
    "\n",
    "new_directory = \"\/data\/notebook_files\/Reddit_Project\"\n",
    "os.chdir(new_directory)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"F60NlQMrfBHeQXBuIq3eUQ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import torch\n",
    "from fer import FER\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datetime import datetime"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zvixurulnlgr5z27sPndIr",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from memes_text_image import (\n",
    "    process_images3, process_dataframe, classify_sentiment, \n",
    "    determine_object_count,  map_to_category,  categorize_time, detect_objects_and_emotions, \n",
    "    extract_text_from_image,classify_category, normalize_upvotes )"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zHdNEXarO5NT8Ul9MzKk3W",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "df_no_duplicates =pd.read_csv('\/data\/notebook_files\/Reddit_Project\/memes_metadata.csv')\n",
    "process_images3(df_no_duplicates, \"\/data\/notebook_files\/Further_Process\/FinalDataset\/Dataset\", \"output_csv\")\n",
    "output101 = pd.read_csv('\/data\/notebook_files\/Reddit_Project\/output_csv')"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"fy0GBV71CbHPAqA1lkBjwI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "new_folder = '\/data\/notebook_files\/Further_Process\/FinalDataset\/Dataset'\n",
    "\n",
    "model = torch.hub.load('ultralytics\/yolov5', 'yolov5s') \n",
    "emotion_model = FER()\n",
    "output101[['Detected Objects', 'Facial Expression']] = output101['Filename'].apply(\n",
    "    lambda filename: pd.Series(detect_objects_and_emotions(model,emotion_model,os.path.join(new_folder, filename)))\n",
    ")\n",
    "output101_with_text = process_dataframe(output101, new_folder)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"snVWyf2L5RhcTavobcj6tU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output101_with_text['Objects_ind'] = output101_with_text['Detected Objects'].apply(lambda x: 0 if pd.isna(x) or x == '' or x.isspace() else 1)\n",
    "output101_with_text['object_count'] = output101_with_text['Detected Objects'].apply(determine_object_count)\n",
    "output101_with_text = pd.get_dummies(output101_with_text, columns=['object_count'], prefix='object_count')\n",
    "output101_with_text['do_category'] = output101_with_text['Detected Objects'].apply(map_to_category)\n",
    "one_hot_encoded = pd.get_dummies(output101_with_text['do_category'], prefix='do_category')\n",
    "output101_with_text = pd.concat([output101_with_text, one_hot_encoded], axis=1)\n",
    "# Replace missing values in 'Facial Expression' with 'unknown'\n",
    "output101_with_text['Facial Expression'].fillna('unknown', inplace=True)\n",
    "output101_with_text = pd.get_dummies(output101_with_text, columns=['Facial Expression'], prefix='Facial_Expression')"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"6gcsLlu30ViQJlvPS68NSN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "output101_with_text['sentiment'] = output101_with_text['Extracted_Text'].apply(classify_sentiment)\n",
    "output101_with_text = pd.get_dummies(output101_with_text, columns=['sentiment'], prefix='meme_text_sentiment')"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"XnU9w8YvOifciAxK6iGsNB",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output101_with_text['UTC'] = output101_with_text['UTC'].apply(lambda x: datetime.utcfromtimestamp(x))\n",
    "output101_with_text['Time'] = output101_with_text['UTC'].apply(categorize_time)\n",
    "output101_with_text = pd.get_dummies(output101_with_text, columns=['Time'], prefix='Time')"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Zw4rpzopIY6edcxEJJwxsB",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "cta_words = ['Comment',  'post', 'View', 'Reply', 'Posted', 'Follow', 'Share',  'Share Save']\n",
    "output101_with_text['call_to_action_ind'] = output101_with_text['Extracted_Text'].apply(lambda text: 1 if text and any(word.lower() in str(text).lower() for word in cta_words) else 0)\n",
    "output101_with_text['submission_text_ind'] = output101_with_text['Submission Text'].notnull().astype(int)\n",
    "output101_with_text['NSFW_ind'] = output101_with_text['NSFW'].astype(int)\n",
    "output101_with_text['title_word_count'] = output101_with_text['Title'].str.split().apply(len)\n"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"xgNBSoXPZsDhJRJzn0ixMh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "text_classifier = pipeline(\"sentiment-analysis\")\n",
    "output101_with_text['title_sentiment'] = output101_with_text['Title'].apply(classify_category)\n",
    "category_one_hot = pd.get_dummies(output101_with_text['title_sentiment'], prefix='title_sentiment')\n",
    "output101_with_text = pd.concat([output101_with_text, category_one_hot], axis=1)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"pyUk3Y7xyzqKIrJa4oka11",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "num_ventiles = 20\n",
    "output101_with_text = output101_with_text.groupby('Subreddit').apply(normalize_upvotes)\n",
    "output101_with_text = list(range(1, num_ventiles + 1))\n",
    "output101_with_text['Ventile'] = pd.qcut(output101_with_text['Normalized_Upvotes'], q=num_ventiles, labels=ventile_labels, duplicates='drop')"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"YwofsnKBupqi7g8EhS2BgY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output101_with_text.to_csv(metadata_with_features.csv)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"oQWoGwGVioLObOqcfJV9Oq",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"minimal",
   "packages":[
    
   ],
   "report_row_ids":[
    
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}
