{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "!pip install --upgrade pip\n",
    "!pip install praw\n",
    "!pip install opencv-python-headless"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"TD1UPdfmqG1WABUhC7jrl4",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from webcolors import hex_to_rgb, CSS3_HEX_TO_NAMES\n",
    "import shutil\n",
    "import os"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"F60NlQMrfBHeQXBuIq3eUQ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "new_directory = \"\/data\/notebook_files\/Reddit_Project\"\n",
    "os.chdir(new_directory)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zvixurulnlgr5z27sPndIr",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from memes_scrapper import download_subreddit_images\n",
    "from preprocessing import process_images\n",
    "from preprocessing import copy_files_with_color_check\n",
    "from preprocessing import copy_images\n",
    "from preprocessing import get_image_formats_distribution"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zHdNEXarO5NT8Ul9MzKk3W",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "\n",
    "def setup_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def process_subreddits(subreddits, project_directory, process_directory):\n",
    "    for subreddit in subreddits:\n",
    "        # Download subreddit images\n",
    "        download_subreddit_images(subreddit)\n",
    "        \n",
    "        # Process images and copy files with color check\n",
    "        input_csv = os.path.join(project_directory, 'downloads', subreddit, f'{subreddit}_metadata.csv')\n",
    "        output_csv = os.path.join(process_directory, f'{subreddit}2.csv')\n",
    "        df = pd.read_csv(input_csv)\n",
    "        process_images(df, output_csv)\n",
    "        \n",
    "        source_folder = os.path.join(project_directory, 'downloads', subreddit)\n",
    "        destination_folder = os.path.join(process_directory, f'{subreddit}_fil')\n",
    "        copy_files_with_color_check(output_csv, source_folder, destination_folder)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"fy0GBV71CbHPAqA1lkBjwI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "project_directory = \"\/data\/notebook_files\/Reddit_Project\"\n",
    "os.chdir(project_directory)\n",
    "\n",
    "process_directory = '\/data\/notebook_files\/Further_Process'\n",
    "setup_directory(process_directory)\n",
    "\n",
    "subreddits = ['wholesomememes', 'dankmemes']\n",
    "#Download memes from a subreddit and store them in their respective subfolders\n",
    "# within the downloads directory. If the directory does not exist, create it. \n",
    "process_subreddits(subreddits, project_directory, process_directory)\n",
    "\n",
    "downloads_directory = \"\/data\/notebook_files\/Reddit_Project\/downloads\""
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"snVWyf2L5RhcTavobcj6tU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "subfolders = [f.path for f in os.scandir(downloads_directory) if f.is_dir()]\n",
    "#Number of images in respective subfolders within the downloads directory.\n",
    "for subfolder in subfolders:\n",
    "        num_images = len([f for f in os.listdir(subfolder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"Folder: {subfolder}, Number of Images: {num_images}\")\n",
    "\n",
    "\n",
    "new_folder = '\/data\/notebook_files\/Further_Process\/FinalDataset\/Dataset'\n",
    "setup_directory(new_folder)\n",
    "copy_images(process_directory, new_folder)\n",
    "print(\"Images copied successfully to the new folder.\")\n",
    "subfolders = [f.path for f in os.scandir(process_directory) if f.is_dir()]\n",
    "for subfolder in subfolders:\n",
    "    num_images = len([f for f in os.listdir(subfolder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"Folder: {subfolder}, Number of Images: {num_images}\")\n",
    "\n",
    "distribution = get_image_formats_distribution(new_folder)\n",
    "print(\"Image Format Distribution:\")\n",
    "for format, count in distribution.items():\n",
    "    print(f\"{format}: {count} images\")\n",
    "\n",
    "dfs = {name: pd.read_csv(f'\/data\/notebook_files\/Further_Process\/{name}2.csv') for name in subreddits}\n",
    "\n",
    "stacked_df = pd.concat(dfs.values(), ignore_index=True)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"6gcsLlu30ViQJlvPS68NSN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "df_filtered2 = stacked_df[stacked_df['Filename'].apply(lambda x: os.path.exists(os.path.join(new_folder, x)))]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "len(df_filtered2)\n",
    "df_filtered2 = df_filtered2.drop(['Dominant Colors', 'Color Names'], axis=1)\n",
    "df_no_duplicates = df_filtered2.drop_duplicates(subset='Filename', keep='first')\n",
    "\n",
    "df_no_duplicates.to_csv('memes_metadata.csv', index=False)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"XnU9w8YvOifciAxK6iGsNB",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"minimal",
   "packages":[
    
   ],
   "report_row_ids":[
    
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}